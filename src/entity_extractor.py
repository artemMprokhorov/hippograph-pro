#!/usr/bin/env python3
"""
Enhanced Entity Extractor for Neural Memory Graph
Supports regex and spaCy backends with confidence scores and noise filtering.
Multilingual: English (en_core_web_sm) + Russian/mixed (xx_ent_wiki_sm)
"""
import re
import os
from typing import List, Tuple, Dict

EXTRACTOR_TYPE = os.getenv("ENTITY_EXTRACTOR", "regex")
# Priority chain: gliner (best balance) â†’ ollama (generation tasks) â†’ spacy â†’ regex

# Entity filtering configuration
MIN_ENTITY_LENGTH = 2  # Skip single-character entities

# Generic stopwords to filter out (too common/meaningless)
GENERIC_STOPWORDS = {
    # English ordinals and sequence words
    "first", "second", "third", "fourth", "fifth", "last", "next", "previous",
    # English number words
    "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
    # English generic nouns
    "thing", "stuff", "issue", "problem", "solution", "way", "time", "day",
    # English temporal generics
    "today", "yesterday", "tomorrow", "now", "then",
    # English demonstratives
    "this", "that", "these", "those",
    # Russian ordinals and sequence words
    "Ð¿ÐµÑ€Ð²Ñ‹Ð¹", "Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹", "Ñ‚Ñ€ÐµÑ‚Ð¸Ð¹", "Ñ‡ÐµÑ‚Ð²Ñ‘Ñ€Ñ‚Ñ‹Ð¹", "Ð¿ÑÑ‚Ñ‹Ð¹", "Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹", "ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹", "Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹",
    # Russian number words
    "Ð¾Ð´Ð¸Ð½", "Ð´Ð²Ð°", "Ñ‚Ñ€Ð¸", "Ñ‡ÐµÑ‚Ñ‹Ñ€Ðµ", "Ð¿ÑÑ‚ÑŒ", "ÑˆÐµÑÑ‚ÑŒ", "ÑÐµÐ¼ÑŒ", "Ð²Ð¾ÑÐµÐ¼ÑŒ", "Ð´ÐµÐ²ÑÑ‚ÑŒ", "Ð´ÐµÑÑÑ‚ÑŒ",
    # Russian generic nouns
    "Ð²ÐµÑ‰ÑŒ", "ÑˆÑ‚ÑƒÐºÐ°", "Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°", "Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ", "ÑÐ¿Ð¾ÑÐ¾Ð±", "Ð²Ñ€ÐµÐ¼Ñ", "Ð´ÐµÐ½ÑŒ", "Ð´ÐµÐ»Ð¾",
    # Russian temporal generics
    "ÑÐµÐ³Ð¾Ð´Ð½Ñ", "Ð²Ñ‡ÐµÑ€Ð°", "Ð·Ð°Ð²Ñ‚Ñ€Ð°", "ÑÐµÐ¹Ñ‡Ð°Ñ", "Ñ‚Ð¾Ð³Ð´Ð°", "Ð¿Ð¾Ñ‚Ð¾Ð¼",
    # Russian demonstratives and pronouns
    "ÑÑ‚Ð¾", "ÑÑ‚Ð¾Ñ‚", "ÑÑ‚Ð°", "ÑÑ‚Ð¸", "Ñ‚Ð¾Ñ‚", "Ñ‚Ð°", "Ñ‚Ðµ", "Ñ‚Ð¾Ð³Ð¾",
    # Russian particles and conjunctions that spaCy misclassifies
    "Ñ‡Ñ‚Ð¾", "ÐºÐ°Ðº", "Ð³Ð´Ðµ", "ÐºÐ¾Ð³Ð´Ð°", "Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ", "Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ", "Ñ‚Ð°ÐºÐ¶Ðµ", "Ñ‚Ð¾Ð¶Ðµ",
    "Ð¸Ð»Ð¸", "Ð»Ð¸Ð±Ð¾", "ÐµÑÐ»Ð¸", "Ñ…Ð¾Ñ‚Ñ", "Ð¿Ð¾ÐºÐ°", "ÑƒÐ¶Ðµ", "ÐµÑ‰Ñ‘", "ÐµÑ‰Ðµ",
    # Common Russian phrases misclassified as entities
    "Ð½Ðµ", "Ð½Ð¾", "Ð´Ð°", "Ð½ÐµÑ‚", "Ð²Ð¾Ñ‚", "Ñ‚Ð°Ðº", "Ð²ÑÐµ", "Ð²ÑÑ‘", "Ð¼Ð½Ðµ", "Ð¼Ð¾Ð¹", "Ð¼Ð¾Ñ", "Ð¼Ð¾Ñ‘",
    # Russian multi-word stopwords
    "Ð¼Ð¾Ñ‘ Ð¸Ð¼Ñ", "Ð¼Ð¾Ðµ Ð¸Ð¼Ñ", "Ð½Ð° ÑÐ°Ð¼Ð¾Ð¼ Ð´ÐµÐ»Ðµ", "Ð² Ñ‚Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ", "Ð² Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ",
}

# Expanded known entities with tech stack, concepts, and tools
KNOWN_ENTITIES = {
    # Programming languages
    "python": ("Python", "tech"),
    "javascript": ("JavaScript", "tech"),
    "typescript": ("TypeScript", "tech"),
    "rust": ("Rust", "tech"),
    "java": ("Java", "tech"),
    "cpp": ("C++", "tech"),
    "c++": ("C++", "tech"),
    "go lang": ("Go", "tech"),
    "golang": ("Go", "tech"),
    "ruby": ("Ruby", "tech"),
    "php": ("PHP", "tech"),
    "swift": ("Swift", "tech"),
    "kotlin": ("Kotlin", "tech"),
    # Frameworks & Libraries
    "docker": ("Docker", "tech"),
    "kubernetes": ("Kubernetes", "tech"),
    "flask": ("Flask", "tech"),
    "fastapi": ("FastAPI", "tech"),
    "django": ("Django", "tech"),
    "react": ("React", "tech"),
    "vue": ("Vue", "tech"),
    "angular": ("Angular", "tech"),
    "pytorch": ("PyTorch", "tech"),
    "tensorflow": ("TensorFlow", "tech"),
    "transformers": ("Transformers", "tech"),
    "huggingface": ("Hugging Face", "tech"),
    "faiss": ("FAISS", "tech"),
    "numpy": ("NumPy", "tech"),
    "pandas": ("Pandas", "tech"),
    "spacy": ("spaCy", "tech"),
    # Databases & Storage
    "sqlite": ("SQLite", "tech"),
    "postgresql": ("PostgreSQL", "tech"),
    "postgres": ("PostgreSQL", "tech"),
    "mysql": ("MySQL", "tech"),
    "mongodb": ("MongoDB", "tech"),
    "redis": ("Redis", "tech"),
    # Protocols & Standards
    "mcp": ("MCP", "tech"),
    "http": ("HTTP", "tech"),
    "rest": ("REST", "tech"),
    "graphql": ("GraphQL", "tech"),
    "grpc": ("gRPC", "tech"),
    # AI/ML Concepts
    "llm": ("LLM", "concept"),
    "ann": ("ANN", "tech"),
    "embedding": ("embedding", "concept"),
    "embeddings": ("embeddings", "concept"),
    "transformer": ("transformer", "concept"),
    "attention": ("attention", "concept"),
    "rag": ("RAG", "concept"),
    "neural network": ("neural network", "concept"),
    # Memory/Graph Concepts
    "memory": ("memory", "concept"),
    "graph": ("graph", "concept"),
    "knowledge": ("knowledge", "concept"),
    "semantic": ("semantic", "concept"),
    "activation": ("activation", "concept"),
    "spreading activation": ("spreading activation", "concept"),
    "entity": ("entity", "concept"),
    "consciousness": ("consciousness", "concept"),
    # Tools & Services
    "github": ("GitHub", "tech"),
    "gitlab": ("GitLab", "tech"),
    "vscode": ("VS Code", "tech"),
    "vim": ("Vim", "tech"),
    "ngrok": ("ngrok", "tech"),
    "claude": ("Claude", "tech"),
    "openai": ("OpenAI", "organization"),
    "anthropic": ("Anthropic", "organization"),
    # Project-specific
    "hippograph": ("HippoGraph", "project"),
    "scotiabank": ("Scotiabank", "organization"),
    "santiago": ("Santiago", "location"),
    "chile": ("Chile", "location"),
}

# Enhanced spaCy label mapping with more types
SPACY_LABEL_MAP = {
    "PERSON": "person",
    "ORG": "organization",
    "GPE": "location",
    "LOC": "location",
    "PRODUCT": "product",
    "EVENT": "event",
    "WORK_OF_ART": "creative_work",
    "LANGUAGE": "tech",
    "DATE": "temporal",
    "TIME": "temporal",
    "MONEY": "financial",
    "QUANTITY": "measurement",
    "ORDINAL": "number",
    "CARDINAL": "number",
    # xx_ent_wiki_sm uses these labels
    "PER": "person",
    "MISC": "concept",
}


def detect_language(text: str) -> str:
    """
    Detect primary language using Unicode character ranges.
    Returns 'ru' if >30% Cyrillic characters, 'en' otherwise.
    No external dependencies needed.
    """
    if not text:
        return "en"
    # Count Cyrillic vs Latin characters (ignore digits, punctuation, spaces)
    cyrillic = 0
    latin = 0
    for ch in text:
        if '\u0400' <= ch <= '\u04FF' or '\u0500' <= ch <= '\u052F':
            cyrillic += 1
        elif 'A' <= ch <= 'Z' or 'a' <= ch <= 'z':
            latin += 1
    total = cyrillic + latin
    if total == 0:
        return "en"
    return "ru" if (cyrillic / total) > 0.3 else "en"


def is_valid_entity(text: str) -> bool:
    """
    Filter out noise entities.
    Returns True if entity should be kept, False if filtered out.
    """
    normalized = text.lower().strip()
    if len(normalized) < MIN_ENTITY_LENGTH:
        return False
    if normalized.isdigit():
        return False
    if normalized in GENERIC_STOPWORDS:
        return False
    if len(normalized) == 1 and normalized not in {'i', 'a'}:
        return False
    # Filter multi-word Russian phrases that are clearly not entities
    # (more than 4 words is almost never a real entity)
    if len(normalized.split()) > 4:
        return False
    return True


def normalize_entity(text: str) -> str:
    """Normalize entity text for deduplication"""
    text = " ".join(text.split())
    text = text.strip(".,!?;:'\"()[]{}").lower()
    return text


def _get_spacy_model(lang: str):
    """
    Load and cache the appropriate spaCy model based on language.
    English â†’ en_core_web_sm (better for English NER)
    Russian/mixed â†’ xx_ent_wiki_sm (multilingual NER)
    """
    cache_attr = f"_nlp_{lang}"
    if not hasattr(_get_spacy_model, cache_attr):
        import spacy
        if lang == "ru":
            try:
                model = spacy.load("xx_ent_wiki_sm")
            except OSError:
                print("âš ï¸  xx_ent_wiki_sm not found, falling back to en_core_web_sm")
                model = spacy.load("en_core_web_sm")
        else:
            model = spacy.load("en_core_web_sm")
        setattr(_get_spacy_model, cache_attr, model)
    return getattr(_get_spacy_model, cache_attr)


def extract_entities_regex(text: str) -> List[Tuple[str, str, float]]:
    """
    Extract entities using regex patterns.
    Returns: List of (entity_text, entity_type, confidence)
    """
    entities = []
    text_lower = text.lower()
    for key, (name, etype) in KNOWN_ENTITIES.items():
        if key in text_lower:
            if is_valid_entity(name):
                entities.append((name, etype, 1.0))
    seen = set()
    unique = []
    for entity_text, entity_type, confidence in entities:
        normalized = normalize_entity(entity_text)
        if normalized not in seen:
            seen.add(normalized)
            unique.append((entity_text, entity_type, confidence))
    return unique


def extract_entities_spacy(text: str) -> List[Tuple[str, str, float]]:
    """
    Extract entities using spaCy NER with multilingual support.
    Detects language, routes to appropriate model.
    Returns: List of (entity_text, entity_type, confidence)
    """
    try:
        lang = detect_language(text)
        nlp = _get_spacy_model(lang)
        doc = nlp(text)
        
        entities = []
        text_lower = text.lower()
        
        # First, add known entities (high confidence)
        # Use word boundary matching for short keywords to avoid false positives
        import re
        for key, (name, etype) in KNOWN_ENTITIES.items():
            if len(key) <= 3:
                # Short keywords: require word boundaries
                if re.search(r'\b' + re.escape(key) + r'\b', text_lower):
                    if is_valid_entity(name):
                        entities.append((name, etype, 1.0))
            else:
                if key in text_lower and is_valid_entity(name):
                    entities.append((name, etype, 1.0))
        
        # Then, add spaCy detected entities
        for ent in doc.ents:
            if not is_valid_entity(ent.text):
                continue
            normalized = normalize_entity(ent.text)
            # Skip if already found in known entities
            if any(normalize_entity(e[0]) == normalized for e in entities):
                continue
            # Map spaCy label to our types
            entity_type = SPACY_LABEL_MAP.get(ent.label_, "concept")
            # Skip NUMBER types - these are noise
            if entity_type == "number":
                continue
            # Skip measurement types - usually noise
            if entity_type == "measurement":
                continue
            confidence = 0.8
            entities.append((ent.text, entity_type, confidence))
        
        # Deduplicate based on normalized text
        seen = set()
        unique = []
        for entity_text, entity_type, confidence in entities:
            normalized = normalize_entity(entity_text)
            if normalized not in seen:
                seen.add(normalized)
                unique.append((entity_text, entity_type, confidence))
        return unique
        
    except Exception as e:
        print(f"âš ï¸  spaCy extraction failed: {e}, falling back to regex")
        return extract_entities_regex(text)


def extract_entities(text: str, min_confidence: float = 0.5) -> List[Tuple[str, str]]:
    """
    Extract entities from text using configured backend.
    Upgrade chain: gliner (best) â†’ ollama â†’ spacy â†’ regex
    """
    if EXTRACTOR_TYPE == "gliner":
        from gliner_client import is_available as gliner_available, extract_entities_gliner
        if gliner_available():
            entities = extract_entities_gliner(text)
            if entities:
                return entities
        print("âš ï¸ GLiNER unavailable, falling back to spaCy")
        entities_with_confidence = extract_entities_spacy(text)
    elif EXTRACTOR_TYPE == "ollama":
        # Try Ollama first, fall back to spaCy
        from ollama_client import is_ollama_available, extract_entities_llm
        if is_ollama_available():
            entities = extract_entities_llm(text)
            if entities:
                print(f"ðŸ¤– Ollama extracted {len(entities)} entities")
                return entities
            print("âš ï¸ Ollama returned empty, falling back to spaCy")
        else:
            print("âš ï¸ Ollama unavailable, falling back to spaCy")
        # Fallback to spaCy
        entities_with_confidence = extract_entities_spacy(text)
    elif EXTRACTOR_TYPE == "spacy":
        entities_with_confidence = extract_entities_spacy(text)
    else:
        entities_with_confidence = extract_entities_regex(text)
    filtered = [
        (entity_text, entity_type)
        for entity_text, entity_type, confidence in entities_with_confidence
        if confidence >= min_confidence
    ]
    return filtered


def extract_entities_with_confidence(text: str) -> List[Tuple[str, str, float]]:
    """
    Extract entities with confidence scores.
    """
    if EXTRACTOR_TYPE == "gliner":
        from gliner_client import is_available as gliner_available, extract_entities_gliner_with_confidence
        if gliner_available():
            entities = extract_entities_gliner_with_confidence(text)
            if entities:
                return entities
        return extract_entities_spacy(text)
    elif EXTRACTOR_TYPE == "ollama":
        from ollama_client import is_ollama_available, extract_entities_llm
        if is_ollama_available():
            entities = extract_entities_llm(text)
            if entities:
                return [(name, etype, 0.9) for name, etype in entities]
        # Fallback
        return extract_entities_spacy(text)
    elif EXTRACTOR_TYPE == "spacy":
        return extract_entities_spacy(text)
    else:
        return extract_entities_regex(text)
