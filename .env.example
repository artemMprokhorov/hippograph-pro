# Neural Memory API Configuration
# Copy this file to .env and fill in your values
# NEVER commit .env to git!

# API Key for MCP access
# Generate strong key: see MCP_CONNECTION.md
NEURAL_API_KEY=your_secure_api_key_here

# Optional: Ngrok authentication for remote access
# Get token from: https://dashboard.ngrok.com/get-started/your-authtoken
# NGROK_AUTH_TOKEN=your_ngrok_token_here

# Optional: Database path (default: /app/data/memory.db)
# DB_PATH=/app/data/memory.db

# Optional: Enable emotional memory fields
# ENABLE_EMOTIONAL_MEMORY=true

# Optional: Blend scoring weights
# Four-signal formula: final = α×semantic + β×spreading + γ×BM25 + δ×temporal
# where β = 1-α-γ-δ (spreading gets remainder)
# BLEND_ALPHA=0.6   # Semantic similarity weight (default: 0.6)
# BLEND_GAMMA=0.15  # BM25 keyword weight (default: 0.0 = disabled)
# BLEND_DELTA=0.1   # Temporal weight (default: 0.0 = disabled, auto-enabled for temporal queries)

# Optional: Fusion method — 'blend' (weighted sum) or 'rrf' (Reciprocal Rank Fusion)
# RRF eliminates score scale mismatch, no weight tuning needed
# FUSION_METHOD=blend

# Optional: Cross-encoder reranking (improves precision, adds ~100ms latency)
# RERANK_ENABLED=true           # Enable reranking (default: false)
# RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2  # Model name
# RERANK_TOP_N=20               # Rerank this many candidates (default: 20)
# RERANK_WEIGHT=0.3             # Reranker score weight vs blend score (default: 0.3)

# ═══════════════════════════════════════════════════════════════
# Embedding Model Configuration
# ═══════════════════════════════════════════════════════════════
# Choose based on your hardware and language needs.
# ⚠️ CHANGING MODEL REQUIRES FULL RE-INDEX of all notes!
#    Run: python3 src/reindex_embeddings.py
#
# Recommended models (from lightest to heaviest):
#
# sentence-transformers/all-MiniLM-L6-v2
#   Dims: 384 | RAM: ~80MB | Speed: fastest | Languages: English only
#   Best for: English-only deployments, minimal hardware (4GB RAM)
#
# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  [DEFAULT]
#   Dims: 384 | RAM: ~120MB | Speed: fast | Languages: 50+
#   Best for: Multilingual on modest hardware (8GB RAM)
#
# BAAI/bge-m3
#   Dims: 1024 | RAM: ~2.2GB | Speed: moderate | Languages: 100+
#   Best for: Maximum quality + multilingual (16GB+ RAM, GPU recommended)
#
# intfloat/multilingual-e5-large
#   Dims: 1024 | RAM: ~2.2GB | Speed: moderate | Languages: 100+
#   Best for: Alternative to bge-m3 with strong multilingual performance
#
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Security Warning:
# - Keep this file private
# - Use strong, unique API keys
# - Rotate keys if exposed
# - Never share credentials publicly

# ═══════════════════════════════════════════════════════════════
# Ollama LLM Integration (optional — Pro feature)
# ═══════════════════════════════════════════════════════════════
# Start with: docker compose --profile ollama up -d
# Then pull a model: docker exec hippograph-ollama ollama pull qwen2.5:7b
#
# OLLAMA_URL=http://ollama:11434
# OLLAMA_MODEL=qwen2.5:7b
#
# To use LLM for entity extraction (better quality, slower):
# ENTITY_EXTRACTOR=ollama
#
# Fusion method: blend (weighted sum, default) or rrf (Reciprocal Rank Fusion)
# FUSION_METHOD=blend
